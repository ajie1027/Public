#                 一、K8S部署（kubeadm方式1.19.16）

## 集群规划

根据具体环境进行对应规划

|     主机名      |      IP       |
| :-------------: | :-----------: |
| br-k8s-master01 | 10.168.90.218 |
| br-k8s-master02 | 10.168.90.141 |
| br-k8s-master03 | 10.168.90.152 |
|  br-k8s-node01  | 10.168.90.216 |
|  br-k8s-node02  | 10.168.90.114 |
|  br-k8s-node03  | 10.168.90.87  |

## 1、初始化服务器

### 1.1 配置机器主机名

在10.168.90.218上执行如下命令

```
hostnamectl set-hostname br-k8s-master01  && bash
```

在10.168.90.141上执行如下命令

```
hostnamectl set-hostname br-k8s-master02  && bash
```

在10.168.90.152上执行如下命令

```
hostnamectl set-hostname br-k8s-master03  && bash
```

在10.168.90.216上执行如下命令

```
hostnamectl set-hostname br-k8s-node01  && bash
```

在10.168.90.114上执行如下命令

```
hostnamectl set-hostname br-k8s-node02  && bash
```

在10.168.90.87上执行如下命令

```
hostnamectl set-hostname br-k8s-node03  && bash
```

###1.2 配置主机hosts文件

修改**`每台`**主机的/etc/hosts文件，增加如下6行

```
10.168.90.218 br-k8s-master01
10.168.90.141 br-k8s-master02
10.168.90.152 br-k8s-master03
10.168.90.216 br-k8s-node01
10.168.90.114 br-k8s-node02
10.168.90.87  br-k8s-node03
```

### 1.3 关闭交换分区swap

在每台主机上都需要进行操作

#### 临时关闭

```
[root@br-k8s-master01 ~]# swapoff -a
```

#### 永久关闭

```
将/etc/fstab中swap这行开头加一下注释
当前环境没有这一行，不用操作
```

#### 确认结果

```
[root@br-k8s-master01 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:          15308        1264       11306         130        2737       13590
Swap:             0           0           0
```

### 1.4 修改主机内核参数

在每台主机上都需要进行操作

```
[root@br-k8s-master01 ~]# modprobe br_netfilter
[root@br-k8s-master01 ~]# echo "modprobe br_netfilter" >> /etc/profile
[root@br-k8s-master01 ~]# cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
[root@br-k8s-master01 ~]# sysctl -p /etc/sysctl.d/k8s.conf
```

### 1.5 关闭firewalld防火墙

在每台主机上都需要进行操作

```
[root@br-k8s-master01 ~]# systemctl stop firewalld ; systemctl disable firewalld
```

### 1.6 关闭 selinux

在每台主机上都需要进行操作

```
[root@br-k8s-master01 ~]# sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
#修改selinux配置文件之后，重启机器，selinux配置才能永久生效
```

验证

```
[root@br-k8s-master01 ~]# getenforce 
Disabled
#显示Disabled说明selinux已经关闭
```

### 1.7 配置 repo 源

在每台主机上都需要进行操作

#### 配置docker的repo源

```
[root@br-k8s-master01 ~]# vim /etc/yum.repos.d/docker.repo 
[dockerce]
baseurl=http://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/7/$basearch/stable
enabled=1
gpgcheck=0
```

#### 配置安装k8s组件需要的repo源

```
[root@br-k8s-master01 ~]#vim /etc/yum.repos.d/kubernetes.repo 
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
```

### 1.8 配置时间同步

在每台主机上都需要进行操作

```
#安装ntpdate命令
[root@br-k8s-master01 ~]# yum install ntpdate -y
#跟网络时间做同步
[root@br-k8s-master01 ~]# ntpdate cn.pool.ntp.org
#把时间同步做成计划任务
[root@br-k8s-master01 ~]# crontab -e
* */1 * * * /usr/sbin/ntpdate  cn.pool.ntp.org		
#用于查看计划任务是否成功
[root@eu-k8s-node03 ~]# crontab -l
* */1 * * * /usr/sbin/ntpdate  cn.pool.ntp.org
```

### 1.9 开启 ipvs

在每台主机上都需要进行操作

```
#把ipvs.modules上传到每台机器的/etc/sysconfig/modules/目录下
[root@br-k8s-master01 ~]# cat /etc/sysconfig/modules/ipvs.modules 
#!/bin/bash
ipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack"
for kernel_module in ${ipvs_modules}; do
 /sbin/modinfo -F filename ${kernel_module} > /dev/null 2>&1
 if [ 0 -eq 0 ]; then
 /sbin/modprobe ${kernel_module}
 fi
done
[root@br-k8s-master01 ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs
```

### 1.10 安装基础软件包

在每台主机上都需要进行操作

```
[root@br-k8s-master01 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 wget net-tools nfs-utils lrzsz gcc gcc-c++ make cmake libxml2-devel openssl-devel curl curl-devel unzip sudo ntp libaio-devel wget vim ncurses-devel autoconf automake zlib-devel  python-devel epel-release openssh-server socat  ipvsadm conntrack ntpdate telnet ipvsadm
```

## 2、安装 docker 服务

在每台主机上都需要进行操作

### 2.1 安装 docker-ce 

```
[root@br-k8s-master01 ~]# yum install docker-ce-20.10.6 docker-ce-cli-20.10.6 containerd.io  -y
```

### 2.2 配置 docker 镜像加速器和驱动

```
#根据具体情况更改registry-mirrors 和 insecure-registries
[root@br-k8s-master01 ~]# vim /etc/docker/daemon.json 
{
 "registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com", "https://rncxm540.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "insecure-registries": ["pub.registry.byd.com","10.168.90.87:1180"]
}
#修改docker文件驱动为systemd，默认为cgroupfs，kubelet默认使用systemd，两者必须一致才可以。

[root@br-k8s-master01 ~]# systemctl daemon-reload  && systemctl restart docker && systemctl status docker
```

## 3、安装初始化k8s需要的软件包

### 3.1 安装软件包

在每台master、node节点上安装以下三个软件包

```
[root@br-k8s-master01 ~]# yum install -y kubelet-1.19.16 kubeadm-1.19.16 kubectl-1.19.16
[root@br-k8s-master01 ~]# systemctl enable kubelet && systemctl start kubelet && systemctl status kubelet

#看到kubelet状态不是running状态，这个是正常的，不用管，等k8s组件起来这个kubelet就正常了。
```

### 3.2 启用kubectl命令的自动补全功能

```
# 安装并配置bash-completion
 
[root@br-k8s-master01 ~]# yum install -y bash-completion
[root@br-k8s-master01 ~]# echo 'source /usr/share/bash-completion/bash_completion' >> /etc/profile
[root@br-k8s-master01 ~]# source /etc/profile
[root@br-k8s-master01 ~]# echo "source <(kubectl completion bash)" >> ~/.bashrc
[root@br-k8s-master01 ~]# source ~/.bashrc
```

## 4、kubeadm初始化k8s集群

### 4.1 创建 kubeadm-config.yaml文件

在 br-k8s-master01 上创建 kubeadm-config.yaml文件

```
[root@br-k8s-master01 ~]# mkdir -p k8s-install/
[root@br-k8s-master01 ~]# cd k8s-install/
[root@br-k8s-master01 k8s-install]# vim kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.19.16
controlPlaneEndpoint: 10.168.90.218:6443
apiServer:
 certSANs:
 - "10.168.90.218"
 - "10.168.90.141"
 - "10.168.90.152"
 - "br-k8s-master01"
 - "br-k8s-master02"
 - "br-k8s-master03"
etcd:
  local:
    serverCertSANs:
      - "10.168.90.218"
      - "10.168.90.141"
      - "10.168.90.152"
      - "br-k8s-master01"
      - "br-k8s-master02"
      - "br-k8s-master03"
    peerCertSANs:
      - "10.168.90.218"
      - "10.168.90.141"
      - "10.168.90.152"
      - "br-k8s-master01"
      - "br-k8s-master02"
      - "br-k8s-master03"
    dataDir: /var/lib/etcd
networking:
  podSubnet: 172.16.0.0/16
  serviceSubnet: 172.18.0.0/16
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind:  KubeProxyConfiguration
mode: ipvs
```

### 4.2 使用kubeadm初始化k8s集群

```
[root@br-k8s-master01 k8s-install]# kubeadm init --config kubeadm-config.yaml

#重置节点命令
kubeadm reset
```

### 4.3 注意事项

#### 4.3.1 获取默认配置文件

```
[root@br-k8s-master01 k8s-install]# kubeadm config print init-defaults --kubeconfig ClusterConfiguration > kubeadm-config.yaml
```

#### 4.3.2 镜像问题

默认是从官方下载镜像，如果网络不好，可提前下载，使用docker load手动导入，此时需要在kubeadm-config.yaml中添加一行

imageRepository: registry.aliyuncs.com/google_containers（根据实际情况更改值）

```
#查看初始化过程中所需要的镜像
[root@br-k8s-master01 k8s-install]# kubeadm config images list --config kubeadm-config.yaml

k8s.gcr.io/kube-apiserver:v1.19.16
k8s.gcr.io/kube-controller-manager:v1.19.16
k8s.gcr.io/kube-scheduler:v1.19.16
k8s.gcr.io/kube-proxy:v1.19.16
k8s.gcr.io/pause:3.2
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns:1.7.0
#下载初始化过程中所需要的镜像
[root@br-k8s-master01 k8s-install]# kubeadm config images pull --config kubeadm-config.yaml
```

显示如下，说明安装完成

![1667010566191](C:\Users\LU41FE~1.QIN\AppData\Local\Temp\1667010566191.png)



```
#配置kubectl的配置文件config，相当于对kubectl进行授权，这样kubectl命令可以使用这个证书对k8s集群进行管理
[root@br-k8s-master01 k8s-install]# mkdir -p $HOME/.kube
[root@br-k8s-master01 k8s-install]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@br-k8s-master01 k8s-install]# chown $(id -u):$(id -g) $HOME/.kube/config
```

## 5、扩容集群-添加master节点

把br-k8s-master01节点的证书拷贝到br-k8s-master02和br-k8s-master03上

```
#证书在/etc/kubernetes/pki/
[root@br-k8s-master01 k8s-install]# ls /etc/kubernetes/pki/
```

在br-k8s-master02创建目录

```
[root@br-k8s-master02 ~]# mkdir -p  /etc/kubernetes/pki/
[root@br-k8s-master02 ~]# kubeadm join 10.168.90.218:6443 --token pnoiyk.8cbtee836debxt4k  --discovery-token-ca-cert-hash sha256:5094aa933e11a06eed16e887ea3592ab5955b03bb81ddef9c5b1c664aafed17f --control-plane
```

在br-k8s-master03创建目录

```
[root@br-k8s-master03 ~]# mkdir -p  /etc/kubernetes/pki/
[root@br-k8s-master03 ~]# kubeadm join 10.168.90.218:6443 --token ndq7nh.8p3hp1516nfw9js9  --discovery-token-ca-cert-hash    sha256:5094aa933e11a06eed16e887ea3592ab5955b03bb81ddef9c5b1c664aafed17f --control-plane

#看到下面说明节点已经加入到集群了
```

![1667011380502](C:\Users\LU41FE~1.QIN\AppData\Local\Temp\1667011380502.png)



如果token过期可在br-k8s-master01上查看

```
[root@br-k8s-master01 k8s-install]# kubeadm token create --print-join-command
```

## 6、扩容集群-添加node节点

在br-k8s-node01、br-k8s-node02、br-k8s-node03上分别执行

```
[root@br-k8s-node01 ~]# kubeadm join 10.168.90.218:6443 --token ndq7nh.8p3hp1516nfw9js9  --discovery-token-ca-cert-hash sha256:5094aa933e11a06eed16e887ea3592ab5955b03bb81ddef9c5b1c664aafed17f

#看到下面说明节点已经加入到集群了
```

![1667011715774](C:\Users\LU41FE~1.QIN\AppData\Local\Temp\1667011715774.png)

可以把node节点上的ROLES变成work，按照如下方法：

```
[root@br-k8s-master01 k8s-install]# kubectl label node br-k8s-node01 node-role.kubernetes.io/worker=worker
[root@br-k8s-master01 k8s-install]# kubectl label node br-k8s-node02 node-role.kubernetes.io/worker=worker
[root@br-k8s-master01 k8s-install]# kubectl label node br-k8s-node03 node-role.kubernetes.io/worker=worker
```

## 7、安装网络组件 flannel

将flannel二进制文件上传到每台主机的/opt/cni/bin目录下，并添加执行权限

```
[root@br-k8s-master01 bin]# pwd
/opt/cni/bin
[root@br-k8s-master01 bin]# chmod +x /opt/cni/bin/flannel
[root@br-k8s-master01 bin]# ll flannel 
-rwxrwxr-x 1 root root 2650368 Oct 22 07:13 flannel
```

上传flannel.yaml到br-k8s-master01上

```
#确认网络配置和kubeadm-config.yaml中的配置一致
[root@br-k8s-master01 k8s-install]# grep -n 172 flannel.yaml 
128:      "Network": "172.16.0.0/16",
[root@br-k8s-master01 k8s-install]# kubectl apply -f flannel.yaml
[root@br-k8s-master01 bin]# ps -ef | grep flannel | grep -v grep
root     28586 28564  0 Oct24 ?        00:03:01 /opt/bin/flanneld --ip-masq --kube-subnet-mgr
```

查看集群情况

```
[root@br-k8s-master01 ~]# kubectl get nodes
NAME              STATUS   ROLES    AGE     VERSION
br-k8s-master01   Ready    master   6d23h   v1.19.16
br-k8s-master02   Ready    master   6d20h   v1.19.16
br-k8s-master03   Ready    master   6d20h   v1.19.16
br-k8s-node01     Ready    worker   6d20h   v1.19.16
br-k8s-node02     Ready    worker   6d20h   v1.19.16
br-k8s-node03     Ready    worker   6d20h   v1.19.16
```

## 8、etcdctl查看

### 8.1 下载etcdctl客户端命令行工具

```
wget https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz
或者上传已有的 etcd-v3.4.14-linux-amd64.tar.gz
```

解压配置

```
[root@br-k8s-master01 tmp]# tar -zxf etcd-v3.4.14-linux-amd64.tar.gz
[root@br-k8s-master01 
tmp]# cp etcd-v3.4.14-linux-amd64/etcdctl /usr/local/bin
[root@br-k8s-master01 tmp]# chmod +x /usr/local/bin/etcdctl
[root@br-k8s-master01 tmp]# etcdctl version
etcdctl version: 3.4.14
API version: 3.4
```

### 8.2 查看etcd高可用集群健康状态

```
[root@br-k8s-master01 tmp]# ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --write-out=table --endpoints=10.168.90.218:2379,10.168.90.141:2379,10.168.90.152:2379 endpoint health
+--------------------+--------+------------+-------+
|      ENDPOINT      | HEALTH |    TOOK    | ERROR |
+--------------------+--------+------------+-------+
| 10.168.90.218:2379 |   true | 8.336581ms |       |
| 10.168.90.141:2379 |   true | 8.933693ms |       |
| 10.168.90.152:2379 |   true | 8.846773ms |       |
+--------------------+--------+------------+-------+
```

### 8.3 查看etcd高可用集群列表

```
[root@br-k8s-master01 tmp]# ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --write-out=table --endpoints=10.168.90.218:2379,10.168.90.141:2379,10.168.90.152:2379 member list
+------------------+---------+-----------------+----------------------------+----------------------------+------------+
|        ID        | STATUS  |      NAME       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER |
+------------------+---------+-----------------+----------------------------+----------------------------+------------+
| 34e2af9e6ad5e0bf | started | br-k8s-master02 | https://10.168.90.141:2380 | https://10.168.90.141:2379 |      false |
| 8fc920b5f5ca5f32 | started | br-k8s-master03 | https://10.168.90.152:2380 | https://10.168.90.152:2379 |      false |
| e37a642a94881579 | started | br-k8s-master01 | https://10.168.90.218:2380 | https://10.168.90.218:2379 |      false |
+------------------+---------+-----------------+----------------------------+----------------------------+------------+
```

### 8.4 查看etcd高可用集群leader

```
[root@br-k8s-master01 tmp]# ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --write-out=table --endpoints=10.168.90.218:2379,10.168.90.141:2379,10.168.90.152:2379 endpoint status
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|      ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 10.168.90.218:2379 | e37a642a94881579 |  3.4.13 |  3.3 MB |      true |      false |        11 |    2332187 |            2332187 |        |
| 10.168.90.141:2379 | 34e2af9e6ad5e0bf |  3.4.13 |  3.3 MB |     false |      false |        11 |    2332187 |            2332187 |        |
| 10.168.90.152:2379 | 8fc920b5f5ca5f32 |  3.4.13 |  3.3 MB |     false |      false |        11 |    2332187 |            2332187 |        |
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```

## 9、测试k8s集群

### 9.1 测试coredns是否正常

可把busybox-1-28.tar.gz上传到每个节点上，手动解压

```
[root@br-k8s-master01 ~]# docker load -i busybox-1-28.tar.gz
[root@br-k8s-master01 ~]# kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh
# 进入容器后执行
/ # nslookup kubernetes.default.svc.cluster.local
Server:    172.18.0.10
Address 1: 172.18.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default.svc.cluster.local
Address 1: 172.18.0.1 kubernetes.default.svc.cluster.local
```

### 9.2 在Kubernetes集群中创建一个pod，然后暴露端口，验证是否正常访问

```
[root@br-k8s-master01 ~]# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
 
[root@br-k8s-master01 ~]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
 
[root@br-k8s-master01 ~]# kubectl get pods,svc
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-554b9c67f9-wf5lm   1/1     Running   0          24s
 
NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   172.18.0.1       <none>        443/TCP        39m
service/nginx        NodePort    172.16.224.251   <none>        80:30328/TCP   9
```

访问地址：http://NodeIP:30328 

## 10、删除master节点上的污点

默认master节点上有taint，不允许pod调度到上面

执行如下命令删除污点

```
[root@br-k8s-master01 ~]# kubectl taint node br-k8s-master01 node-role.kubernetes.io/master-
[root@br-k8s-master01 ~]# kubectl taint node br-k8s-master02 node-role.kubernetes.io/master-
[root@br-k8s-master01 ~]# kubectl taint node br-k8s-master03 node-role.kubernetes.io/master-
```

检查

```
[root@br-k8s-master01 ~]# kubectl describe nodes br-k8s-master01 | grep -i  taint
Taints:             <none>
[root@br-k8s-master01 ~]# kubectl describe nodes br-k8s-master02 | grep -i  taint
Taints:             <none>
[root@br-k8s-master01 ~]# kubectl describe nodes br-k8s-master03 | grep -i  taint
Taints:             <none>
```

## 11、增加证书到期时间

### 11.1 查看证书到期时间

证书默认到期时间是1年

```
[root@br-k8s-master01 ~]# kubeadm alpha certs check-expiration
```

### 11.2 延长证书过期时间

把update-kubeadm-cert.sh文件上传到br-k8s-master01、br-k8s-master02、br-k8s-master03节点

```
update-kubeadm-cert.sh文件所在的github地址如下：
https://github.com/luckylucky421/kubernetes1.17.3
```

在每个master节点都执行如下命令

```
chmod +x update-kubeadm-cert.sh
./update-kubeadm-cert.sh all
```

再次查看证书到期时间，都已增加到了10年

```
[root@br-k8s-master01 ~]# kubeadm alpha certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Oct 19, 2032 07:43 UTC   9y                                      no      
apiserver                  Oct 19, 2032 07:43 UTC   9y              ca                      no      
apiserver-etcd-client      Oct 19, 2032 07:43 UTC   9y              etcd-ca                 no      
apiserver-kubelet-client   Oct 19, 2032 07:43 UTC   9y              ca                      no      
controller-manager.conf    Oct 19, 2032 07:43 UTC   9y                                      no      
etcd-healthcheck-client    Oct 19, 2032 07:43 UTC   9y              etcd-ca                 no      
etcd-peer                  Oct 19, 2032 07:43 UTC   9y              etcd-ca                 no      
etcd-server                Oct 19, 2032 07:43 UTC   9y              etcd-ca                 no      
front-proxy-client         Oct 19, 2032 07:43 UTC   9y              front-proxy-ca          no      
scheduler.conf             Oct 19, 2032 07:43 UTC   9y                                      no      

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Oct 19, 2032 03:26 UTC   9y              no      
etcd-ca                 Oct 19, 2032 03:26 UTC   9y              no      
front-proxy-ca          Oct 19, 2032 03:26 UTC   9y              no 
```

## 12、部署kubernetes-dashboard

上传dashboard-onlyread.yaml和dashboard.yaml到br-k8s-master01上

```
[root@br-k8s-master01 ~]# kubectl apply -f dashboard-onlyread.yaml
[root@br-k8s-master01 ~]# kubectl apply -f dashboard.yaml
```

查看kubernetes-dashboard命名空间下的secrets

```
[root@br-k8s-master01 k8s-install]# kubectl get -n kubernetes-dashboard secrets 
NAME                                        TYPE                                  DATA   AGE
default-token-gx628                         kubernetes.io/service-account-token   3      6d19h
kubernetes-dashboard-certs                  Opaque                                0      6d19h
kubernetes-dashboard-csrf                   Opaque                                1      6d19h
kubernetes-dashboard-key-holder             Opaque                                2      6d19h
kubernetes-dashboard-readonly-token-wmn6d   kubernetes.io/service-account-token   3      4d19h
kubernetes-dashboard-token-6xzvp            kubernetes.io/service-account-token   3      6d19h
```

查看读写token

```
[root@br-k8s-master01 k8s-install]# kubectl describe -n kubernetes-dashboard secrets kubernetes-dashboard-token-6xzvp
```

查看只读token

```
[root@br-k8s-master01 k8s-install]# kubectl describe -n kubernetes-dashboard secrets kubernetes-dashboard-readonly-token-wmn6d
```

查看访问端口

```
[root@br-k8s-master01 k8s-install]# kubectl get svc -n kubernetes-dashboard 
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   172.18.159.17    <none>        8000/TCP        6d19h
kubernetes-dashboard        NodePort    172.18.237.201   <none>        443:32443/TCP   6d19h
```

在windows系统访问  https://ip:32443

## 13、部署traefik

将traefik部署在br-k8s-node01、br-k8s-node02节点上

在br-k8s-node01、br-k8s-node02节点上添加如下标签

```
[root@br-k8s-master01 k8s-install]# kubectl label node br-k8s-node01  traefik-role=traefik
[root@br-k8s-master01 k8s-install]# kubectl label node br-k8s-node02  traefik-role=traefik
```

上传traefik-role-admin.yaml、traefik-abc.yaml、traefik.yaml文件到br-k8s-master01上

```
[root@br-k8s-master01 k8s-install]# kubectl apply -f traefik-abc.yaml 
[root@br-k8s-master01 k8s-install]# kubectl apply -f traefik-role-admin.yaml 
[root@br-k8s-master01 k8s-install]# kubectl apply -f traefik.yaml 
```

查看pod信息

```
[root@br-k8s-master01 k8s-install]# kubectl get pod -n kube-system -o wide | grep traefik
traefik-rzdx7                             1/1     Running   1          6d19h   10.168.90.114   br-k8s-node02     <none>           <none>
traefik-ttjjp                             1/1     Running   1          6d19h   10.168.90.216   br-k8s-node01     <none>           <none>
```

查看traefik监听端口

```
[root@br-k8s-node01 ~]# netstat -lnput | grep 80
tcp6       0      0 :::8080                 :::*                    LISTEN      25183/traefik       
tcp6       0      0 :::8000                 :::*                    LISTEN      25183/traefik  

[root@br-k8s-node02 ~]# netstat -lnput | grep 80
tcp6       0      0 :::8000                 :::*                    LISTEN      26783/traefik       
tcp6       0      0 :::8080                 :::*                    LISTEN      26783/traefik    
```

访问10.168.90.114:8080或者10.168.90.216:8080 查看traefik管理页面

------

# 二、挂载对象存储

上传mount_s3_buckert.sh脚本到需要挂载的主机上，执行命令

```
sh mount_s3_buckert.sh
```

```
#!/bin/bash
file_passwd=/etc/passwd-s3fs
#按实际信息更改
ak=
#按实际信息更改
sk=
file_s3fs_path=/opt/s3fs/bin/s3fs
dir_s3fs_path=/opt/s3fs
dir_mount=/bucket-obs
#按实际信息更改
url="https://s3.sa-east-1.amazonaws.com/"
#按实际信息更改
obs_name=br-ota-s3
cmd="/opt/s3fs/bin/s3fs ${obs_name} ${dir_mount} -o url=${url} -o passwd_file=${file_passwd} -o umask=022 -o allow_other"
#install dependence software
yum install git automake fuse fuse-devel gcc-c++ libcurl-devel libxml2-devel make openssl-devel -y > /dev/null
#install s3fs
echo -e "\033[47;30m ===========================Install s3fs=================================== \033[0m"
if [ ! -f ${file_s3fs_path} ];then
    cd /tmp
    git clone https://github.com/s3fs-fuse/s3fs-fuse.git
    cd /tmp/s3fs-fuse/
    sh autogen.sh
    mkdir ${dir_s3fs_path} -p
    chmod 755 configure
    ./configure --prefix=${dir_s3fs_path}
    make && make install
    if [ $? == 0 ];then
        echo -e "\033[32m install s3fs-fuse sucess! \033[0m"
    else
        echo -e "\033[31m install s3fs-fuse fail! Please check it out!!! \033[0m"
        exit 123
    fi
else
    echo -e "\033[33m s3fs is already installed. \033[0m"
fi
echo -e "\033[47;30m ===========================Generate ${file_passwd}========================== \033[0m"
if [ ! -f ${file_passwd} ];then
cat >>${file_passwd}<<EOF
${ak}:${sk}
EOF
echo -e "\033[32m generate ${file_passwd} sucess. \033[0m"
else
    echo -e "\033[33m ${file_passwd} is already exist. \033[0m"
fi
#cat ${file_passwd}
echo -e "\033[47;30m ===========================Mount ${dir_mount}================================ \033[0m"
chmod 600 /etc/passwd-s3fs
if [ ! -d ${dir_mount} ];then
    mkdir ${dir_mount}
    ${cmd}
    if [ $? == 0 ];then
        echo -e "\033[32m mount ${dir_mount} sucess. \033[0m"
    else
        echo -e "\033[31m mount ${dir_mount} fail!!! \033[0m"
    fi
else
    echo -e "\033[33m Directory ${dir_mount} is already exist! \033[0m"
fi
df -h ${dir_mount}
echo -e "\033[47;30m ===========================Config /etc/rc.local=================================== \033[0m"
sed -i '/allow_other/d' /etc/rc.local
echo "${cmd}" >> /etc/rc.local
chmod +x /etc/rc.local
cat -n  /etc/rc.local
ls -l /etc/rc.local

```

 查看挂载点信息

```
[root@br-k8s-master01 ~]# df -h /bucket-obs/
Filesystem      Size  Used Avail Use% Mounted on
s3fs             16E     0   16E   0% /bucket-obs
```

